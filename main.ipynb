{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae09933",
   "metadata": {},
   "source": [
    "Academic Honesty Statement\n",
    "\n",
    "> *We declare that this submission is our own work, and that we did not use any pretrained model or code that we did not explicitly cite.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b748d1",
   "metadata": {},
   "source": [
    "## 0. Libraries Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c555857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    mean_absolute_error,\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Check keras version\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f7cba",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "### 1.1. Problem Statement\n",
    "The objective of this project is to design, train, and evaluate a Deep Learning model capable of solving a **Multi-task Learning** problem. We are provided with a custom dataset containing **3,000 samples**. Each sample consists of a `(32, 32)` input matrix (interpreted as a grayscale image) associated with three distinct targets:\n",
    "\n",
    "* **Target A:** Classification (10 classes, integers 0-9).\n",
    "* **Target B:** Classification (32 classes, integers 0-31).\n",
    "* **Target C:** Regression (Real value in range $[0, 1]$).\n",
    "\n",
    "### 1.2. Methodology\n",
    "Our goal is to build a **single unified Neural Network** (likely based on CNN) that predicts all three components simultaneously. This approach allows the model to learn shared feature representations from the input images before branching out to specific tasks.\n",
    "\n",
    "The notebook is structured as follows:\n",
    "1.  **Data Inspection:** Analyzing shapes and distributions.\n",
    "2.  **Model Design:** Creating a multi-head architecture.\n",
    "3.  **Experiments:** Tuning hyperparameters (Learning Rate).\n",
    "4.  **Evaluation:** Training the final model and analyzing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d738f0",
   "metadata": {},
   "source": [
    "# 2. Dataset inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ca969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. DATASET INSPECTION\n",
    "# ==========================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Load the dataset\n",
    "try:\n",
    "    with np.load(\"dataset_dev_3000.npz\") as data:\n",
    "        X_raw = data[\"X\"]\n",
    "        y_raw = data[\"y\"]\n",
    "    print(f\"Dataset loaded successfully.\")\n",
    "    print(f\"   X Shape: {X_raw.shape}\")\n",
    "    print(f\"   y Shape: {y_raw.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"WARNING: 'dataset_dev_3000.npz' not found. Generating dummy data.\")\n",
    "    X_raw = np.random.uniform(0, 7, (3000, 32, 32)).astype(\"float32\")\n",
    "    y_raw = np.column_stack([\n",
    "        np.random.randint(0, 10, 3000), \n",
    "        np.random.randint(0, 32, 3000), \n",
    "        np.random.rand(3000)\n",
    "    ]).astype(\"float32\")\n",
    "\n",
    "# 2. Basic Inspection\n",
    "print(\"\\n--- Raw Data Statistics ---\")\n",
    "print(f\"Min Value: {X_raw.min():.4f}\")\n",
    "print(f\"Max Value: {X_raw.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8901f53",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing\n",
    "\n",
    "### 3.1. Train/validation split strategy\n",
    "To maximize the amount of data available for training while ensuring a fair evaluation, we updated our splitting strategy with `random_state=32` for reproducibility. We employed a **two-stage split**:\n",
    "\n",
    "1.  **Hold-out Test Set (10%):** We set aside 10% of the raw data (**300 samples**) as the final Test set. This set is strictly isolated and used only for the final evaluation.\n",
    "2.  **Train / Validation Split (90/10 of remaining):** From the remaining 2,700 samples, we split further:\n",
    "    * **Validation Set:** 10% of the remaining data (~**270 samples**) to monitor overfitting during training.\n",
    "    * **Training Set:** The remaining ~**2,430 samples**.\n",
    "\n",
    "**Final Data Distribution:**\n",
    "* **Training:** 2,430 samples (~81%) $\\rightarrow$ *Increased to improve model learning.*\n",
    "* **Validation:** 270 samples (~9%)\n",
    "* **Test:** 300 samples (~10%)\n",
    "\n",
    "### 3.2. Preprocessing: Normalization & Reshaping\n",
    "Based on the data inspection, we applied the following transformations to prepare inputs for the CNN:\n",
    "\n",
    "1.  **Reshaping:** The input tensor was reshaped from `(N, 32, 32)` to `(N, 32, 32, 1)` to add the channel dimension required by Keras `Conv2D` layers.\n",
    "2.  **Normalization:** We observed that the pixel values were not in the standard [0, 255] range but had a global maximum of **6.8486**. We normalized the data to the range **$[0, 1]$** using the formula:\n",
    "    $$X_{norm} = \\frac{X}{6.8486}$$\n",
    "    \n",
    "    * **Train Norm Range:** $[0.000, 1.000]$\n",
    "    * This ensures stable gradient descent and faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af72720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3.1. TRAIN/VALIDATION SPLIT STRATEGY\n",
    "# ==========================================\n",
    "\n",
    "# Split 1: Separate Test set (10%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_raw, y_raw, test_size=0.1, random_state=32\n",
    ")\n",
    "\n",
    "# Split 2: Separate Validation set (10% of remaining)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.1, random_state=32\n",
    ")\n",
    "\n",
    "print(f\"Train Set: {X_train.shape}\")\n",
    "print(f\"Val Set:   {X_val.shape}\")\n",
    "print(f\"Test Set:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3.2. DATA PREPARATION & NORMALIZATION\n",
    "# ==========================================\n",
    "print(\"--- DATA PREPARATION ---\")\n",
    "\n",
    "# 1. Calculate Global Max from Training Data\n",
    "GLOBAL_MAX = float(X_train.max())\n",
    "print(f\"Global Max found in training data: {GLOBAL_MAX:.4f}\")\n",
    "\n",
    "# 2. Reshape & Normalize\n",
    "# Add channel dimension: (N, 32, 32) -> (N, 32, 32, 1)\n",
    "# Normalize to range [0, 1] using Global Max\n",
    "X_train_norm = X_train.reshape((-1, 32, 32, 1)).astype(\"float32\") / GLOBAL_MAX\n",
    "X_val_norm = X_val.reshape((-1, 32, 32, 1)).astype(\"float32\") / GLOBAL_MAX\n",
    "X_test_norm = X_test.reshape((-1, 32, 32, 1)).astype(\"float32\") / GLOBAL_MAX\n",
    "\n",
    "# Verify shapes and ranges\n",
    "print(f\"Train Norm Shape: {X_train_norm.shape} (Rank-4)\")\n",
    "print(f\"Train Norm Range: [{X_train_norm.min():.3f}, {X_train_norm.max():.3f}]\")\n",
    "\n",
    "# 3. Split Targets for Multi-Output Model\n",
    "y_train_A, y_train_B, y_train_C = y_train[:, 0], y_train[:, 1], y_train[:, 2]\n",
    "y_val_A, y_val_B, y_val_C = y_val[:, 0], y_val[:, 1], y_val[:, 2]\n",
    "y_test_A, y_test_B, y_test_C = y_test[:, 0], y_test[:, 1], y_test[:, 2]\n",
    "\n",
    "print(\"Data preparation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7552d07",
   "metadata": {},
   "source": [
    "###  Data Augmentation Strategy\n",
    "\n",
    "Given the limited dataset size (3,000 samples), overfitting is a significant concern where the model might memorize training examples instead of learning general features. To mitigate this, we implemented an **Online Data Augmentation** pipeline integrated directly into the model.\n",
    "\n",
    "We selected specific transformations suited for low-resolution images ($32 \\times 32$):\n",
    "\n",
    "1.  **RandomFlip (\"horizontal\"):** * Simulates mirror reflections. This teaches the model that an object facing left or right is still the same object (e.g., a car or animal).\n",
    "    \n",
    "2.  **RandomTranslation (0.05, 0.05):** * Shifts the image horizontally and vertically by up to 5%.\n",
    "    * *Reasoning:* In real-world data, objects are not always perfectly centered. This layer forces the model to learn **translation invariance**, recognizing features regardless of their slight position shifts.\n",
    "\n",
    "3.  **RandomRotation (0.05):** * Rotates the image by approximately $18^\\circ$ (5% of a full circle).\n",
    "    * *Reasoning:* We kept the rotation factor small (`0.05`) because rotating low-resolution images too aggressively can cause interpolation artifacts and loss of critical details.\n",
    "\n",
    "*Note: These layers are active only during training (`training=True`) and are automatically disabled during validation and testing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb81049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# DATA AUGMENTATION & PIPELINE\n",
    "# ==========================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"--- BUILDING TF.DATA PIPELINE ---\")\n",
    "\n",
    "# 1. Define Data Augmentation Policy\n",
    "# Note: Using standard geometric augmentations.\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomTranslation(0.05, 0.05),\n",
    "    layers.RandomRotation(0.05),\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "print(\"   Augmentation layers initialized.\")\n",
    "\n",
    "# 2. Create Train Dataset\n",
    "# Wrap tensors into a tf.data.Dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train_norm, (y_train_A, y_train_B, y_train_C))\n",
    ")\n",
    "\n",
    "def train_map(x, y):\n",
    "    \"\"\"\n",
    "    Apply augmentation to the input image batch during training.\n",
    "    training=True ensures augmentation is active.\n",
    "    \"\"\"\n",
    "    x = data_augmentation(x, training=True)\n",
    "    return x, y\n",
    "\n",
    "# Configure pipeline: Shuffle -> Map (Augment) -> Batch -> Prefetch\n",
    "train_ds = (\n",
    "    train_ds\n",
    "    .shuffle(1024)\n",
    "    .map(train_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# 3. Create Validation Dataset\n",
    "# No augmentation for validation, just batching\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_val_norm, (y_val_A, y_val_B, y_val_C))\n",
    ").batch(32).prefetch(tf.data.AUTOTUNE) # Added prefetch for speed\n",
    "\n",
    "print(\"Data Pipeline Ready.\")\n",
    "print(f\" Train Batches: {len(train_ds)}\")\n",
    "print(f\" Val Batches:   {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66796ca",
   "metadata": {},
   "source": [
    "# 4. Model architecture reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de8fb4",
   "metadata": {},
   "source": [
    "**Residual Block Architecture**\n",
    "\n",
    "The core building unit of the proposed model is a residual block, which consists of two consecutive 3×3 convolutional layers followed by batch normalization. The first convolution is followed by a ReLU activation, while the second convolution omits the activation until after the residual addition. A shortcut (identity) connection bypasses these convolutional layers and is added element-wise to the block output. This skip connection allows the network to preserve low-level features while learning higher-level representations, improving both convergence speed and generalization.\n",
    "\n",
    "When spatial resolution or channel dimensionality changes, the shortcut path is adapted using a 1×1 convolution with appropriate stride. This ensures that the dimensions of the shortcut and the main path match before addition, maintaining mathematical consistency while enabling downsampling within the residual block.\n",
    "\n",
    "**CIFAR-Style ResNet Trunk**\n",
    "\n",
    "The model adopts a CIFAR-style ResNet design, which is specifically tailored for small input images (32×32 pixels). Unlike ImageNet-style ResNets that use large initial convolutions and aggressive early downsampling, this architecture begins with a lightweight 3×3 convolutional stem and progressively increases feature depth through stacked residual blocks. Downsampling is performed gradually using stride-2 convolutions inside residual blocks, preserving spatial information critical for low-resolution inputs.\n",
    "\n",
    "The trunk is organized into three stages with increasing numbers of filters (32, 64, and 128), allowing the network to capture hierarchical features ranging from local textures to more abstract patterns. This design balances representational power with computational efficiency and reduces the risk of overfitting.\n",
    "\n",
    "**Global Feature Aggregation**\n",
    "\n",
    "Following the convolutional trunk, Global Average Pooling is applied to collapse the spatial dimensions of the feature maps. This operation replaces traditional flattening, dramatically reducing the number of trainable parameters and acting as a strong regularizer. Global pooling forces the network to encode spatially invariant, semantically meaningful features, which is particularly beneficial for preventing overfitting on small datasets.\n",
    "\n",
    "**Shared Representation and Regularization**\n",
    "\n",
    "The pooled features are passed through a shared fully connected layer that produces a compact latent representation used by all task-specific heads. Batch normalization and ReLU activation stabilize training and improve non-linearity, while dropout is applied to further reduce overfitting by randomly deactivating neurons during training. This shared representation enables efficient multi-task learning, allowing the network to leverage common features across different prediction objectives.\n",
    "\n",
    "**Multi-Head Output Design**\n",
    "\n",
    "The architecture employs three specialized output heads, each optimized for a different task. Two classification heads produce probability distributions over 10 and 32 classes respectively using softmax activation, while a regression head outputs a continuous value using a linear activation. Each head consists of task-specific dense layers with batch normalization, ReLU activation, and dropout to ensure sufficient capacity while maintaining regularization. This design allows the model to learn both shared and task-specific representations effectively within a unified framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71437a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4.1. MODEL DEFINITION\n",
    "# ==========================================\n",
    "def residual_block(x, filters, downsample=False):\n",
    "    stride = 2 if downsample else 1\n",
    "    shortcut = x\n",
    "\n",
    "    x = layers.Conv2D(filters, 3, strides=stride, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, 3, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Match dimensions if needed\n",
    "    if downsample or shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, 1, strides=stride, use_bias=False)(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    inputs = keras.Input(shape=(32, 32, 1), name=\"img_input\")\n",
    "\n",
    "    # Stem\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # ResNet stages\n",
    "    x = residual_block(x, 32)\n",
    "    x = residual_block(x, 32)\n",
    "\n",
    "    x = residual_block(x, 64, downsample=True)\n",
    "    x = residual_block(x, 64)\n",
    "\n",
    "    x = residual_block(x, 128, downsample=True)\n",
    "    x = residual_block(x, 128)\n",
    "\n",
    "    # Global pooling\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Shared feature layer\n",
    "    x = layers.Dense(128, use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    features = x\n",
    "\n",
    "    # =============================\n",
    "    # Heads\n",
    "    # =============================\n",
    "\n",
    "    # Head A\n",
    "    a = layers.Dense(64, use_bias=False)(features)\n",
    "    a = layers.BatchNormalization()(a)\n",
    "    a = layers.ReLU()(a)\n",
    "    a = layers.Dropout(0.3)(a)\n",
    "    out_a = layers.Dense(10, activation=\"softmax\", name=\"out_a\")(a)\n",
    "\n",
    "    # Head B\n",
    "    b = layers.Dense(64, use_bias=False)(features)\n",
    "    b = layers.BatchNormalization()(b)\n",
    "    b = layers.ReLU()(b)\n",
    "    b = layers.Dropout(0.3)(b)\n",
    "    out_b = layers.Dense(32, activation=\"softmax\", name=\"out_b\")(b)\n",
    "\n",
    "    # Head C\n",
    "    c = layers.Dense(32, use_bias=False)(features)\n",
    "    c = layers.BatchNormalization()(c)\n",
    "    c = layers.ReLU()(c)\n",
    "    c = layers.Dropout(0.2)(c)\n",
    "    out_c = layers.Dense(1, activation=\"linear\", name=\"out_c\")(c)\n",
    "\n",
    "    return keras.Model(inputs, [out_a, out_b, out_c])\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7823848",
   "metadata": {},
   "source": [
    "The model was compiled using the Adam optimizer with a learning rate of 0.001, which provides adaptive learning rates and stable convergence for deep neural networks. Three task-specific loss functions were used: sparse categorical cross-entropy for the two classification outputs and mean squared error for the regression output. To balance multi-task learning, loss weights of 1.0, 1.0, and 0.1 were applied to the respective outputs, preventing the regression loss from dominating the total objective. Model performance was monitored using accuracy for classification tasks and mean absolute error for the regression task, providing interpretable and task-appropriate evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4.2. COMPILATION\n",
    "# ==========================================\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        \"out_a\": \"sparse_categorical_crossentropy\",\n",
    "        \"out_b\": \"sparse_categorical_crossentropy\",\n",
    "        \"out_c\": \"mse\",\n",
    "    },\n",
    "    loss_weights=[1.0, 1.0, 0.1],\n",
    "    metrics={\"out_a\": \"accuracy\", \"out_b\": \"accuracy\", \"out_c\": \"mae\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920190e3",
   "metadata": {},
   "source": [
    "The model was trained using a maximum of 100 epochs with validation performance monitored at the end of each epoch. Training was conducted using a dedicated training dataset, while a separate validation dataset was used to assess generalization performance and guide model selection. To ensure that the best-performing model was retained, a ModelCheckpoint callback was employed to save the model weights corresponding to the minimum validation loss observed during training. This approach guarantees that the final model represents the optimal trade-off across all output tasks.\n",
    "\n",
    "To improve training stability and convergence, a ReduceLROnPlateau callback was applied. This mechanism automatically reduces the learning rate by a factor of 0.5 when validation loss fails to improve for 20 consecutive epochs. By adaptively lowering the learning rate, the optimizer is encouraged to escape plateaus and refine the solution during later training stages, while preventing premature convergence.\n",
    "\n",
    "Together, these callbacks provide effective regularization and optimization control, reducing the risk of overfitting and improving overall model robustness. The combination of extended training duration, adaptive learning rate adjustment, and validation-based checkpointing ensures that the model converges efficiently while maintaining strong generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4.3. TRAINING\n",
    "# ==========================================\n",
    "print(\"\\n--- 3. STARTING TRAINING ---\")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.h5\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "\n",
    "    # If LR is small, progress per epoch can be slow/noisy; let it run longer and\n",
    "    # optionally auto-reduce LR on plateaus before giving up.\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=20,\n",
    "        verbose=1,\n",
    "        min_lr=1e-4,\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=100,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ede8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4.4. PREDICT FUNCTION\n",
    "# ==========================================\n",
    "def predict_fn(X_raw):\n",
    "    # 1. Reshape\n",
    "    # 2. Normalize\n",
    "    X_proc = X_raw.reshape((-1, 32, 32, 1)).astype(\"float32\") / GLOBAL_MAX\n",
    "\n",
    "    # 3. Predict\n",
    "    model = keras.models.load_model(\"best_model.h5\", compile=False)\n",
    "    pred_probs_a, pred_probs_b, pred_c_val = model.predict(X_proc, verbose=0)\n",
    "\n",
    "    # 4. Process\n",
    "    pred_a = np.argmax(pred_probs_a, axis=1)\n",
    "    pred_b = np.argmax(pred_probs_b, axis=1)\n",
    "    pred_c = pred_c_val.flatten()\n",
    "\n",
    "    # 5. Stack\n",
    "    return np.column_stack([pred_a, pred_b, pred_c])\n",
    "\n",
    "\n",
    "# print(\"\\n--- CHECKING PREDICT_FN ---\")\n",
    "# sample_preds = predict_fn(X_val[:5])\n",
    "# print(\"Output Shape:\", sample_preds.shape)\n",
    "# print(\"Sample Predictions:\\n\", sample_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f2151",
   "metadata": {},
   "source": [
    "The testing results highlight clear differences in performance across the three prediction tasks, reflecting the varying levels of difficulty and data complexity associated with each output head. For **Target A (digits 0–9)**, the model achieves an overall accuracy of 37%, with macro-averaged precision, recall, and F1-scores all around 0.35. Performance varies notably across individual classes. Digits such as 6, 5, and 9 exhibit relatively higher recall and F1-scores, indicating that the model is better at identifying these patterns. In contrast, digits like 0, 3, and 7 show substantially lower recall, suggesting frequent misclassification. The confusion matrix further confirms that several digits are confused with visually similar classes, which is expected in low-resolution grayscale inputs and limited training data.\n",
    "\n",
    "For **Target B (32-class classification)**, performance is considerably weaker, with an overall accuracy of only 5% and macro-averaged F1-score of 0.03. Many classes record zero precision and recall, meaning the model fails to correctly predict these labels altogether. This outcome indicates severe class confusion and insufficient discriminative capacity for this task. The large number of classes, combined with limited per-class samples and shared feature representations, significantly increases task difficulty. The confusion matrix reveals widespread misclassification, with predictions concentrated in a small subset of classes rather than evenly distributed.\n",
    "\n",
    "In contrast, **Target C (regression)** demonstrates more stable behavior. The mean absolute error (MAE) indicates moderate predictive accuracy, and the true-versus-predicted scatter plot shows a general positive correlation between predictions and ground truth values. While predictions deviate from the ideal diagonal line, especially near the extremes, the regression head benefits from shared learned features and a smoother loss landscape.\n",
    "\n",
    "Overall, the results suggest that the model learns coarse visual representations sufficient for simpler classification and regression tasks, but struggles with fine-grained multi-class discrimination. These findings motivate future improvements through data augmentation, class balancing, deeper architectures, or task-specific feature separation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c3f7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4.5. EVALUATION & VISUALIZATION\n",
    "# ==========================================\n",
    "print(\"\\n--- 4. EVALUATION & PLOTS ---\")\n",
    "\n",
    "# 1. Generate Predictions on Test Set\n",
    "# We use the predict_fn wrapper which handles shaping/normalization automatically\n",
    "print(\"Generating predictions for Test Set...\")\n",
    "val_preds = predict_fn(X_test)\n",
    "\n",
    "# Extract components from the stacked predictions\n",
    "# pred_a/b are class indices (integers), pred_c is float\n",
    "pred_a = val_preds[:, 0].astype(int)\n",
    "pred_b = val_preds[:, 1].astype(int)\n",
    "pred_c = val_preds[:, 2]\n",
    "\n",
    "# --- Target A (Classification 0-9) ---\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"TARGET A (Digits 0-9) PERFORMANCE\")\n",
    "print(\"=\" * 40)\n",
    "print(classification_report(y_test_A, pred_a))\n",
    "\n",
    "# Plot Confusion Matrix for A\n",
    "fig_a, ax_a = plt.subplots(figsize=(8, 8))\n",
    "cm_a = confusion_matrix(y_test_A, pred_a)\n",
    "disp_a = ConfusionMatrixDisplay(confusion_matrix=cm_a, display_labels=np.arange(10))\n",
    "disp_a.plot(cmap=plt.cm.Blues, ax=ax_a)\n",
    "ax_a.set_title(\"Target A: Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# --- Target B (Classification 0-31) ---\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"TARGET B (Classes 0-31) PERFORMANCE\")\n",
    "print(\"=\" * 40)\n",
    "# Note: Zero_division=0 handles classes that might not appear in validation set\n",
    "print(classification_report(y_test_B, pred_b, zero_division=0))\n",
    "\n",
    "# Plot Confusion Matrix for B\n",
    "fig_b, ax_b = plt.subplots(figsize=(12, 12))\n",
    "cm_b = confusion_matrix(y_test_B, pred_b)\n",
    "# We don't list all 32 labels on axis to keep it clean, or we can just use default ints\n",
    "disp_b = ConfusionMatrixDisplay(confusion_matrix=cm_b)\n",
    "disp_b.plot(cmap=plt.cm.Greens, ax=ax_b, values_format=\"d\")  # 'd' for integers\n",
    "ax_b.set_title(\"Target B: Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# --- Target C (Regression 0-1) ---\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"TARGET C (Regression) PERFORMANCE\")\n",
    "print(\"=\" * 40)\n",
    "mae_c = mean_absolute_error(y_test_C, pred_c)\n",
    "print(f\"Mean Absolute Error (MAE): {mae_c:.4f}\")\n",
    "\n",
    "# Plot True vs Predicted\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test_C, pred_c, alpha=0.5, color=\"purple\", label=\"Predictions\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\", linewidth=2, label=\"Ideal Perfect Prediction\")\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(f\"Target C: True vs Predicted (MAE={mae_c:.4f})\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ad304",
   "metadata": {},
   "source": [
    "# 5. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a78820",
   "metadata": {},
   "source": [
    "## 5.1.Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152a92a1",
   "metadata": {},
   "source": [
    "### 5.1. Experiment 1 Results: Learning Rate Analysis\n",
    "\n",
    "| Learning Rate | Total Val Loss | Target A (Acc) | Target B (Acc) | Target C (MAE) | Epochs Run |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| **LR = 1e-2 (0.01)** | 0.xxxx | 0.xx | 0.xx | 0.xxxx | ... |\n",
    "| **LR = 1e-3 (0.001)** | 0.xxxx | 0.xx | 0.xx | 0.xxxx | ... |\n",
    "| **LR = 1e-4 (0.0001)** | 0.xxxx | 0.xx | 0.xx | 0.xxxx | ... |\n",
    "\n",
    "**Observation:** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e67ed0",
   "metadata": {},
   "source": [
    "### 5.2. Experiment 2 Results: Scheduler Strategy Analysis\n",
    "\n",
    "| Scheduler Strategy | Total Val Loss | Target A (Acc) | Target B (Acc) | Target C (MAE) |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **1. Fixed LR** (Baseline) | 0.xxxx | 0.xx | 0.xx | 0.xxxx |\n",
    "| **2. ReduceLR On Plateau** | **0.xxxx** | **0.xx** | **0.xx** | **0.xxxx** |\n",
    "\n",
    "**Observation:**\n",
    "* **Fixed LR:** \n",
    "* **ReduceLR:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b40c3",
   "metadata": {},
   "source": [
    "### 5.3.Ablations Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac4bbd4",
   "metadata": {},
   "source": [
    "| Loss Function                     | Accuracy | Precision | Recall | F1-score |\n",
    "|----------------------------------|----------|-----------|--------|----------|\n",
    "| Sparse Categorical Crossentropy | 0.37     | 0.37      | 0.36   | 0.35     |\n",
    "| Sparse Categorical Crossentropy | 0.05     | 0.04      | 0.05   | 0.03     |\n",
    "| Mean Squared Error (MSE)        | N/A      | N/A       | N/A    | N/A      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31949b9f",
   "metadata": {},
   "source": [
    "| Optimizer | Accuracy | Precision | Recall | F1-score |\n",
    "|-----------|----------|-----------|--------|----------|\n",
    "| Adam | 0.37 | 0.37 | 0.36 | 0.35 |\n",
    "| Adam | 0.05 | 0.04 | 0.05 | 0.03 |\n",
    "| Adam | N/A  | N/A  | N/A  | N/A  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159b0ba",
   "metadata": {},
   "source": [
    "# 6. Evaluation & discussion*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f01248",
   "metadata": {},
   "source": [
    "# 7. Reflection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
